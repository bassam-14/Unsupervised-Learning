{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095e2660",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4538f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_euclidean(a, b):\n",
    "    return np.sqrt(np.sum((a - b) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4647aebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_distances(X):\n",
    "    n = X.shape[0]\n",
    "    distances= np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            distances[i, j] = calculate_euclidean(X[i], X[j])\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02368ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "## functions expectes predicted_labels to be a vector/array with size equal \n",
    "# to the number of points and each entry has the assigned cluster corresponding to this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca07a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# silhouette s core -- > How well each data point \n",
    "# fits within its assigned cluster compared to other clusters.\n",
    "# ranges from -1 to 1\n",
    "# it measuers cohesion between points in same cluster and seperation between\n",
    "# points in diff. clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3c3ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a is cohesion\n",
    "# b is seperation\n",
    "\n",
    "def silhouette_score(X, predicted_labels):\n",
    "    D = pairwise_distances(X)\n",
    "    unique_labels = np.unique(predicted_labels)\n",
    "    n = len(predicted_labels)\n",
    "    s = np.zeros(n)\n",
    "\n",
    "    for i in range(n):\n",
    "        same_cluster = predicted_labels == predicted_labels[i]\n",
    "        # other_clusters = predicted_labels != predicted_labels[i]\n",
    "\n",
    "        a = np.mean(D[i, same_cluster]) if np.sum(same_cluster) > 1 else 0\n",
    "\n",
    "        b = np.inf\n",
    "\n",
    "        # calculate seperation (mean sum of distances) \n",
    "        # between our current cluster and all other cluster and take the min\n",
    "        for label in unique_labels:\n",
    "            if label != predicted_labels[i]:\n",
    "                mask = predicted_labels == label\n",
    "                b = min(b, np.mean(D[i, mask]))\n",
    "\n",
    "        s[i] = (b - a) / max(a, b)\n",
    "\n",
    "    return np.mean(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217d80cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Davies–Bouldin Index \n",
    "# What it measures:\n",
    "# Average similarity between each cluster and its most similar cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4c50c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kol ma kant asghar kant ahsn\n",
    "# because i am dividing within cluster scatter by between clusters seperation\n",
    "# so i want the seperation (den.) to be high and scatter within cluster (num.) to be low\n",
    "\n",
    "def davies_bouldin_index(X, predicted_labels):\n",
    "    unique_labels = np.unique(predicted_labels)\n",
    "    k = len(unique_labels)\n",
    "\n",
    "    # mean of each diff. cluster\n",
    "    for label in unique_labels:\n",
    "        centroids = np.array([\n",
    "            X[predicted_labels == label].mean(axis=0) \n",
    "        ])\n",
    "\n",
    "    # we calc. the spread of points around cluster mean (for each diff. cluster)\n",
    "    S = np.zeros(k)         \n",
    "    for i, label in enumerate(unique_labels):\n",
    "        cluster = X[predicted_labels == label]\n",
    "        for point in cluster:\n",
    "            S[i] = np.mean([calculate_euclidean(point, centroids[i])])\n",
    "\n",
    "    R = np.zeros((k, k))\n",
    "    for i in range(k):\n",
    "        for j in range(k):\n",
    "            if i != j:\n",
    "                R[i, j] = (S[i] + S[j]) / calculate_euclidean(centroids[i], centroids[j])\n",
    "\n",
    "    # calc. mean for highest overlap with other clusters\n",
    "    return np.mean(np.max(R, axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f646001d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calinski–Harabasz Index \n",
    "# what it measures:\n",
    "# ratio of between-cluster dispersion to within-cluster dispersion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532c29fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# higher is better as within cluster despersion is in denmo.\n",
    "# its used for choosing k --> as u increase k, within cluster despersion decrease\n",
    "# so higher chi\n",
    "def calinski_harabasz_index(X, predicted_labels):\n",
    "    n, d = X.shape\n",
    "    unique_labels = np.unique(predicted_labels)\n",
    "    k = len(unique_labels)\n",
    "\n",
    "    overall_mean = X.mean(axis=0)\n",
    "\n",
    "    B = 0  # between-cluster dispersion\n",
    "    W = 0  # within-cluster dispersion\n",
    "\n",
    "    for label in unique_labels:\n",
    "        cluster = X[predicted_labels == label]\n",
    "        cluster_mean = cluster.mean(axis=0)\n",
    "        # one sum for all cluster and within each cluster sum, we sum over all points\n",
    "        B += len(cluster) * np.sum((cluster_mean - overall_mean) ** 2)\n",
    "        W += np.sum((cluster - cluster_mean) ** 2)\n",
    "\n",
    "    return (B / (k - 1)) / (W / (n - k))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffab3a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# within cluster sum of squares\n",
    "def wcss(X, predicted_labels):\n",
    "    total = 0\n",
    "    for label in np.unique(predicted_labels):\n",
    "        cluster = X[predicted_labels == label]\n",
    "        centroid = cluster.mean(axis=0)\n",
    "        total += np.sum((cluster - centroid) ** 2)\n",
    "    return total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2beaa949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ranges from -1 to 1 where higher is better\n",
    "def adjusted_rand_index(y_true, y_pred):\n",
    "    n = len(y_true)\n",
    "    labels_true = np.unique(y_true)\n",
    "    labels_pred = np.unique(y_pred)\n",
    "\n",
    "    contingency = np.zeros((len(labels_true), len(labels_pred)))\n",
    "\n",
    "    for i, lt in enumerate(labels_true):\n",
    "        for j, lp in enumerate(labels_pred):\n",
    "            contingency[i, j] = np.sum((y_true == lt) & (y_pred == lp))\n",
    "\n",
    "    def comb2(x):\n",
    "        return x * (x - 1) / 2\n",
    "\n",
    "    sum_comb = np.sum(comb2(contingency))\n",
    "    sum_rows = np.sum(comb2(contingency.sum(axis=1)))\n",
    "    sum_cols = np.sum(comb2(contingency.sum(axis=0)))\n",
    "\n",
    "    expected = sum_rows * sum_cols / comb2(n)\n",
    "    max_index = 0.5 * (sum_rows + sum_cols)\n",
    "\n",
    "    return (sum_comb - expected) / (max_index - expected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9728acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_mutual_info(y_true, y_pred):\n",
    "    n = len(y_true)\n",
    "    labels_true = np.unique(y_true)\n",
    "    labels_pred = np.unique(y_pred)\n",
    "\n",
    "    MI = 0\n",
    "    for lt in labels_true:\n",
    "        for lp in labels_pred:\n",
    "            p_xy = np.sum((y_true == lt) & (y_pred == lp)) / n\n",
    "            if p_xy > 0:\n",
    "                p_x = np.sum(y_true == lt) / n\n",
    "                p_y = np.sum(y_pred == lp) / n\n",
    "                MI += p_xy * np.log(p_xy / (p_x * p_y))\n",
    "\n",
    "    def entropy(labels):\n",
    "        H = 0\n",
    "        for l in np.unique(labels):\n",
    "            p = np.sum(labels == l) / n\n",
    "            H -= p * np.log(p)\n",
    "        return H\n",
    "\n",
    "    return MI / ((entropy(y_true) + entropy(y_pred)) / 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3deb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purity\n",
    "# What it measures:\n",
    "# Extent to which each cluster contains points from a single class.\n",
    "# but this means that if a cluster has smaller no. of points \n",
    "# it will have higher prob. of better purity that is not necessary true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b4b8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# measuers the count of points of the majority class in each cluster / total points\n",
    "# y_true --> ground truth table (labels) for each point\n",
    "# y_pred --> no. of cluster assigned for each point\n",
    "def purity_score(y_true, y_pred):\n",
    "    total = 0\n",
    "    for cluster in np.unique(y_pred):\n",
    "        labels_in_cluster = y_true[y_pred == cluster]  # getting the actual labels of points assigned to same current cluster\n",
    "        counts = np.bincount(labels_in_cluster)         # counts no. of apperances of each non-ve no.  (index==no., value == apperances)\n",
    "        total += np.max(counts)             # total of majority contributions\n",
    "    return total / len(y_true)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
